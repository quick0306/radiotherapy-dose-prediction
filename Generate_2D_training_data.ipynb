{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import pydicom\n",
    "import numpy as np\n",
    "# import dicom_numpy\n",
    "from os import listdir\n",
    "from scipy.io import loadmat\n",
    "#from scipy.misc import imread, imresize, imsave\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset_path, section_size = (128, 256), validation_size = 0.2, save_npy = False, dataset_save_path = 'Data_v4/npy_dataset', batch_size = 1, augmentation = False, normalization=False):\n",
    "    # Create dateset:\n",
    "    data_folder = dataset_path+'/'\n",
    "    data_dirs = listdir(data_folder)\n",
    "    scans = []\n",
    "    dose_imgs = []\n",
    "    i = 0\n",
    "    for data in data_dirs:\n",
    "        if not os.path.isdir(data_folder+data):\n",
    "            f = loadmat(data_folder+data)\n",
    "            scan = f['structset_2d_channel']\n",
    "            dose_img = f['doseset_2d']\n",
    "            dose_img = np.expand_dims(dose_img,axis=3)\n",
    "            scan = np.array(scan, dtype='float32')\n",
    "            dose_img = np.array(dose_img).astype('float32')\n",
    "            if normalization:\n",
    "                print('start normalization')\n",
    "                mask = scan>0\n",
    "                mask_Lung = np.squeeze(mask[:,:,:,7]) ## 7 is the lung dose\n",
    "                mask_Lung = mask_Lung.flatten()\n",
    "                dose_flat = dose_img.flatten()\n",
    "                dose_masked =  np.ma.masked_where(mask_Lung==False, dose_flat)\n",
    "                dose_Lung = dose_masked.compressed()\n",
    "                print('orignal lung mean dose',dose_Lung.mean())\n",
    "                dose_img = dose_img/dose_Lung.mean()*8.0\n",
    "                \n",
    "            print('scan shape and dose shape=',scan.shape, dose_img.shape)\n",
    "            if(i==0):\n",
    "                scans = scan\n",
    "                dose_imgs = dose_img\n",
    "                i = 1\n",
    "            else:\n",
    "                print('add new patient data')\n",
    "                scans= np.concatenate((scans,scan),axis=0)\n",
    "                dose_imgs=np.concatenate((dose_imgs,dose_img),axis=0)\n",
    "            if augmentation:\n",
    "                print('start augmentation')\n",
    "                scan_flip = np.flip(scan,2)\n",
    "                dose_img_flip = np.flip(dose_img,2)\n",
    "                scans= np.concatenate((scans,scan_flip),axis=0)\n",
    "                dose_imgs=np.concatenate((dose_imgs,dose_img_flip),axis=0)\n",
    "    \n",
    "\n",
    "    print('Scan Data Shape: ' + str(scans.shape))\n",
    "    print('Segmantation Data Shape: ' + str(dose_imgs.shape))\n",
    "    if not os.path.exists(dataset_save_path):\n",
    "        os.makedirs(dataset_save_path)\n",
    "    if save_npy:\n",
    "        np.save(dataset_save_path+'/structures.npy', scans)\n",
    "        np.save(dataset_save_path+'/dose.npy', dose_imgs)\n",
    "        print('NPY dataset saved!')\n",
    "        \n",
    "    for batch_i in range(0, dose_imgs.shape[0], batch_size):\n",
    "        batch_npy = np.concatenate((scans[batch_i:batch_i+batch_size],dose_imgs[batch_i:batch_i+batch_size]),axis=3)\n",
    "        batch_npy = np.array(batch_npy)\n",
    "        np.save(dataset_save_path+'/batch_{0}.npy'.format(batch_i), batch_npy)\n",
    "  #  X, X_test, Y, Y_test = train_test_split(scans, dose_imgs, test_size=validation_size, random_state=42)\n",
    "  #  print('Train Data Shape: ' + str(X.shape[0]))\n",
    "  #  print('Test Data Shape: ' + str(X_test.shape[0]))\n",
    "    return scans, dose_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_npy_dataset(npy_dataset_path, split_npy_dataset_path, validation_path, batch_size, test_size):\n",
    "    X = np.load(npy_dataset_path+'/structures.npy')\n",
    "    Y = np.load(npy_dataset_path+'/dose.npy')\n",
    "\n",
    "    if not os.path.exists(split_npy_dataset_path):\n",
    "        os.makedirs(split_npy_dataset_path)\n",
    "    if not os.path.exists(validation_path):\n",
    "        os.makedirs(validation_path)\n",
    "\n",
    "    X, X_test, Y, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
    "    print('X_test size',X_test.shape,'Y_test size=',Y_test.shape)\n",
    "    test_npy = np.concatenate((X_test,Y_test),axis=3)\n",
    "    test_npy = np.array(test_npy)\n",
    "\n",
    "   # np.save(test_path+'/test.npy', test_npy)\n",
    "    for batch_i in range(0, Y_test.shape[0], batch_size):\n",
    "        batch_npy = np.concatenate((X_test[batch_i:batch_i+batch_size],Y_test[batch_i:batch_i+batch_size]),axis=3)\n",
    "        batch_npy = np.array(batch_npy)\n",
    "        np.save(validation_path+'/batch_{0}.npy'.format(batch_i), batch_npy)\n",
    "\n",
    "    for batch_i in range(0, Y.shape[0], batch_size):\n",
    "        batch_npy = np.concatenate((X[batch_i:batch_i+batch_size],Y[batch_i:batch_i+batch_size]),axis=3)\n",
    "        batch_npy = np.array(batch_npy)\n",
    "        np.save(split_npy_dataset_path+'/batch_{0}.npy'.format(batch_i), batch_npy)\n",
    "\n",
    "        \n",
    "    print('Splitted NPY Dataset saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start normalization\n",
      "orignal lung mean dose 7.8809934\n",
      "scan shape and dose shape= (28, 128, 256, 13) (28, 128, 256, 1)\n",
      "start normalization\n",
      "orignal lung mean dose 7.765043\n",
      "scan shape and dose shape= (29, 128, 256, 13) (29, 128, 256, 1)\n",
      "add new patient data\n",
      "start normalization\n",
      "orignal lung mean dose 7.938748\n",
      "scan shape and dose shape= (30, 128, 256, 13) (30, 128, 256, 1)\n",
      "add new patient data\n",
      "start normalization\n",
      "orignal lung mean dose 8.091753\n",
      "scan shape and dose shape= (29, 128, 256, 13) (29, 128, 256, 1)\n",
      "add new patient data\n",
      "start normalization\n",
      "orignal lung mean dose 7.9635015\n",
      "scan shape and dose shape= (29, 128, 256, 13) (29, 128, 256, 1)\n",
      "add new patient data\n",
      "start normalization\n",
      "orignal lung mean dose 7.905461\n",
      "scan shape and dose shape= (27, 128, 256, 13) (27, 128, 256, 1)\n",
      "add new patient data\n",
      "start normalization\n",
      "orignal lung mean dose 8.273853\n",
      "scan shape and dose shape= (30, 128, 256, 13) (30, 128, 256, 1)\n",
      "add new patient data\n",
      "start normalization\n",
      "orignal lung mean dose 7.794336\n",
      "scan shape and dose shape= (31, 128, 256, 13) (31, 128, 256, 1)\n",
      "add new patient data\n",
      "start normalization\n",
      "orignal lung mean dose 7.8238826\n",
      "scan shape and dose shape= (28, 128, 256, 13) (28, 128, 256, 1)\n",
      "add new patient data\n",
      "start normalization\n",
      "orignal lung mean dose 7.953546\n",
      "scan shape and dose shape= (31, 128, 256, 13) (31, 128, 256, 1)\n",
      "add new patient data\n",
      "start normalization\n",
      "orignal lung mean dose 7.8992796\n",
      "scan shape and dose shape= (28, 128, 256, 13) (28, 128, 256, 1)\n",
      "add new patient data\n",
      "start normalization\n",
      "orignal lung mean dose 7.5733876\n",
      "scan shape and dose shape= (29, 128, 256, 13) (29, 128, 256, 1)\n",
      "add new patient data\n",
      "Scan Data Shape: (349, 128, 256, 13)\n",
      "Segmantation Data Shape: (349, 128, 256, 1)\n",
      "start normalization\n",
      "orignal lung mean dose 7.8741274\n",
      "scan shape and dose shape= (30, 128, 256, 13) (30, 128, 256, 1)\n",
      "start normalization\n",
      "orignal lung mean dose 7.9836783\n",
      "scan shape and dose shape= (25, 128, 256, 13) (25, 128, 256, 1)\n",
      "add new patient data\n",
      "start normalization\n",
      "orignal lung mean dose 7.864896\n",
      "scan shape and dose shape= (28, 128, 256, 13) (28, 128, 256, 1)\n",
      "add new patient data\n",
      "Scan Data Shape: (83, 128, 256, 13)\n",
      "Segmantation Data Shape: (83, 128, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset_path = 'Data_v4'\n",
    "    npy_training_path = 'Data_v4/npy_dataset/training_npy_dataset'\n",
    "    npy_validation_path = 'Data_v4/npy_dataset/validation_npy'\n",
    "    npy_dataset_path ='Data_v4'\n",
    "    training_path = 'Data_v4/training'\n",
    "    validation_path = 'Data_v4/validation'\n",
    "\n",
    "    scans, dose_imgs = get_dataset(training_path, section_size = (128, 256, 1), validation_size = 0.2, save_npy = False, dataset_save_path = npy_training_path, batch_size=1,augmentation=False, normalization=True)\n",
    "    scans, dose_imgs = get_dataset(validation_path, section_size = (128, 256, 1), validation_size = 0.2, save_npy = False, dataset_save_path = npy_validation_path, batch_size=1,augmentation=False, normalization=True)\n",
    " #   split_npy_dataset(npy_dataset_path, splitted_npy_dataset_path, validation_path, batch_size = 1, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = get_scan('Data/structset.mat')\n",
    "test2  = get_dose_img('Data/doseset.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, Y, Y_test = train_test_split(test1, test2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_array.shape\n",
    "dose_array=np.expand_dims(dose_array,axis=3)\n",
    "print(voxel_array.shape)\n",
    "print(dose_array.shape)\n",
    "test=np.concatenate((voxel_array,dose_array),axis=3)\n",
    "np.array(test)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scan(mat_path):\n",
    "    # Getting structure set images from path:\n",
    "    if not os.path.exists(mat_path):\n",
    "        print('MAT files not exists!')\n",
    "        return\n",
    "\n",
    "    f = loadmat('Data/structset_2d_channel.mat')\n",
    "    voxel_array = f['structset_2d_channel']\n",
    "   # voxel_array = np.expand_dims(voxel_array,axis=3)\n",
    "    return voxel_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dose_img(images_path):\n",
    "    # Getting dose image from file\n",
    "    if not os.path.exists(images_path):\n",
    "        print('Dose images not exists!')\n",
    "        return\n",
    "\n",
    "    f = loadmat('Data/doseset_2d.mat')\n",
    "    dose = f['doseset_2d']\n",
    "    dose = np.expand_dims(dose,axis=3)\n",
    "    return dose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_pading(scan, seg_img, section_size):\n",
    "    # For easly split:\n",
    "    pad_size = section_size - (scan.shape[-1] % section_size)\n",
    "    if pad_size != section_size:\n",
    "        padded_scan = np.pad(scan, ((0,0),(0,0),(0,pad_size)), 'constant')\n",
    "        try:\n",
    "            padded_seg_img = np.pad(seg_img, ((0,0),(0,0),(0,pad_size)), 'constant')\n",
    "        except:\n",
    "            padded_seg_img = None\n",
    "    else:\n",
    "        padded_scan = scan\n",
    "        padded_seg_img = seg_img\n",
    "    return padded_scan, padded_seg_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_scans_imgs(scans, seg_img, section_size):\n",
    "    # Split with sliding window:\n",
    "    splitted_scans = []\n",
    "    for i in range(0, scans.shape[-1]-(section_size-1)):\n",
    "        splitted_scans.append(scans[:,:,i:i+section_size])\n",
    "\n",
    "    splitted_seg_img = []\n",
    "    for i in range(0, seg_img.shape[-1]-(section_size-1)):\n",
    "        splitted_seg_img.append(seg_img[:,:,i:i+section_size])\n",
    "\n",
    "    splitted_scans = np.array(splitted_scans)\n",
    "    splitted_seg_img = np.array(splitted_seg_img)\n",
    "    return splitted_scans, splitted_seg_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npy_dataset(npy_dataset_path, test_size = 0.2):\n",
    "    X = np.load(npy_dataset_path+'/structures.npy')\n",
    "    Y = np.load(npy_dataset_path+'/dose.npy')\n",
    "    X, X_test, Y, Y_test = train_test_split(X, Y, test_size=test_size)\n",
    "    print('Train Data Shape: ' + str(X.shape[0]))\n",
    "    print('Test Data Shape: ' + str(X_test.shape[0]))\n",
    "    return X, X_test, Y, Y_test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
